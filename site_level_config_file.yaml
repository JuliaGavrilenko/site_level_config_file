global_variables:
  - &spark_hadoop_master_ip_address 188.184.95.231
  - &spark_hadoop_master_fqdn spark-master.cern.ch
  - &spark_hadoop_worker_ip_address 188.184.104.92
  - &spark_hadoop_worker_fqdn spark-worker.cern.ch


spark_worker_runtime_variables:
  - &spark_hadoop_worker_runtime_var_spark_hadoop_master_fqdn
    __from__: *spark_hadoop_master_fqdn

preferred_tech_stack:
  level_1_configuration: puppet
  level_2_configuration: sh
  container_orchestration: docker-swarm
  container: docker

site_infrastructure:
  - fqdn: *spark_hadoop_master_fqdn
    ip_address: *spark_hadoop_master_ip_address
  - fqdn: *spark_hadoop_worker_fqdn
    ip_address: *spark_hadoop_worker_ip_address


lightweight_components:
  - type: spark_hadoop_master
    name: spark-hadoop-master
    repository_url: "https://github.com/maany/simple_spark_cluster_master"
    repository_revision: "master"
    execution_id: 1
    deploy:
      - node: *spark_hadoop_master_fqdn
        container_count: 1
    config:
        enable_init_daemon: false
        spark_driver_memory: "1g"
        spark_executor_memory: "1g"
        spark_yarn_am_memory: "512m"
        spark_history_fs_log_directory: "hdfs://spark-master.cern.ch:9000/spark-logs"
        spark_history_fs_update_interval: "30s"



  - name: spark-hadoop-worker
    type: spark_hadoop_worker
    repository_url: "https://github.com/maany/simple_spark_cluster_worker"
    repository_revision: "master"
    execution_id: 0
    deploy:
      - node: *spark_hadoop_worker_fqdn
        container_count: 1
    config:
        enable_init_daemon: false
        spark_master: *spark_hadoop_master_fqdn
        spark_driver_memory: "1g"
        spark_executor_memory: "1g"
        spark_yarn_am_memory: "512m"
        spark_history_fs_log_directory: "hdfs://spark-master.cern.ch:9000/spark-logs"
