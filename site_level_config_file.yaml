global_variables:
  - &spark_hadoop_master_ip_address 188.184.91.176
  - &spark_hadoop_master_fqdn spark-hadoop-master-0.cern.ch
#  - &spark_hadoop_worker_ip_address 188.184.88.69
#  - &spark_hadoop_worker_fqdn spark-hadoop-worker-0.cern.ch
#  - &spark_hadoop_add_worker_1_ip_address 188.184.83.48
#  - &spark_hadoop_add_worker_1_fqdn spark-hadoop-add-1-worker.cern.ch
#  - &spark_hadoop_add_worker_2_ip_address 188.185.76.205
#  - &spark_hadoop_add_worker_2_fqdn spark-hadoop-add-2-worker.cern.ch
#  - &spark_hadoop_add_worker_3_ip_address 188.184.86.181
#  - &spark_hadoop_add_worker_3_fqdn spark-hadoop-add-3-worker.cern.ch
  - &spark_hadoop_add_worker_4_ip_address 188.185.71.78
  - &spark_hadoop_add_worker_4_fqdn spark-hadoop-submit.cern.ch

spark_worker_runtime_variables:
  - &spark_hadoop_worker_runtime_var_spark_hadoop_master_fqdn
    __from__: *spark_hadoop_master_fqdn

preferred_tech_stack:
  level_1_configuration: puppet
  level_2_configuration: sh
  container_orchestration: docker-swarm
  container: docker

site_infrastructure:
  - fqdn: *spark_hadoop_master_fqdn
    ip_address: *spark_hadoop_master_ip_address
#  - fqdn: *spark_hadoop_worker_fqdn
#    ip_address: *spark_hadoop_worker_ip_address
#  - fqdn: *spark_hadoop_add_worker_1_fqdn
#    ip_address: *spark_hadoop_add_worker_1_ip_address
#  - fqdn: *spark_hadoop_add_worker_2_fqdn
#    ip_address: *spark_hadoop_add_worker_2_ip_address
#  - fqdn: *spark_hadoop_add_worker_3_fqdn
#    ip_address: *spark_hadoop_add_worker_3_ip_address
  - fqdn: *spark_hadoop_add_worker_4_fqdn
    ip_address: *spark_hadoop_add_worker_4_ip_address

lightweight_components:
  - name: spark-hadoop-master
    type: spark_hadoop_master
    repository_url: "https://github.com/maany/simple_spark_cluster_master"
    repository_revision: "master"
    execution_id: 1
    deploy:
      - node: *spark_hadoop_master_fqdn
        container_count: 1
    config:

  - name: spark-hadoop-worker
    type: spark_hadoop_worker
    repository_url: "https://github.com/maany/simple_spark_cluster_worker"
    repository_revision: "master"
    execution_id: 0
    deploy:
#      - node: *spark_hadoop_worker_fqdn
#        container_count: 1
#      - node: *spark_hadoop_add_worker_1_fqdn
#        container_count: 1
#      - node: *spark_hadoop_add_worker_2_fqdn
#        container_count: 1
#      - node: *spark_hadoop_add_worker_3_fqdn
#        container_count: 1
      - node: *spark_hadoop_add_worker_4_fqdn
        container_count: 1
    config:
#        enable_init_daemon: false
#        spark_master: *spark_hadoop_master_fqdn
#        spark_driver_memory: "1g"
#        spark_executor_memory: "1g"
#        spark_yarn_am_memory: "512m"
#        spark_history_fs_log_directory: "hdfs://spark-master.cern.ch:9000/spark-logs"

